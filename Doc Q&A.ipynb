{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32ec82b",
   "metadata": {},
   "source": [
    "### This script focuses on doing Doc Q&A using doc embeddings, LLMs and even storing of said embeddings in Vector Stores. We explore a solution using GPT from Open AI and a completely open source solution too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b7a11",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "**This document assumes you have a some understanding about the following:**\n",
    "* Python and Object Oriented Programming\n",
    "* NLP concepts such as embeddings\n",
    "* somewhat know how LLMs work\n",
    "* know a bit about VectorDBs\n",
    "\n",
    "**Explicitly used packages:**\n",
    "* Langchain\n",
    "* Transformers\n",
    "\n",
    "**Implicitly used packages:**\n",
    "* ChromaDB\n",
    "* openai\n",
    "* sentence-transformers\n",
    "\n",
    "\n",
    "Implicit packages: These packages must be installed to allow for the explicitly defined packages to perform some functions.\n",
    "The explicitly used packages implement the functionality of these implicit packages and simplify it for the end user by abstracting many complicated lines of code into one simple function call (read: Wrapper Classes) but you do not need to understand the inner workings of the implicit packages if you are a beginner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986ed43",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Create an OPEN AI account and set up your Open AI API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a099b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"enter your OPEN AI API Key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35e6cc",
   "metadata": {},
   "source": [
    "Importing OpenAI related classes from Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c476d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37a4a1",
   "metadata": {},
   "source": [
    "\n",
    "Let us start by creating an object of the OpenAI class and set some parameters. Here, we set up the parameter called temperature. Having a lower temperature value ensures that our LLM output is deterministic in nature and not too random and \"creative\".\n",
    "\n",
    "Let us call the instance (object) of the OpenAI class \"llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed83d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c602f9",
   "metadata": {},
   "source": [
    "Let us define a variable text in which we will store the question we will be asking GPT\n",
    "The llm object takes a string as input and returns the response from the GPT model as output. We can print the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0df4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Socktastic.\n"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa463979",
   "metadata": {},
   "source": [
    "****\n",
    "Now that we have an answer from the GPT model, let us move onto loading a Doc into a DB and then running a query over it.\n",
    "The pipeline for this task is as follows:\n",
    "\n",
    "1. Load doc\n",
    "2. split lengthy docs \n",
    "3. get doc embeddings \n",
    "4. store doc embeddings in vector db (chroma db)\n",
    "5. query over the db to obtain the correct chunk to answer from\n",
    "6. provide this chunk along with your question to obtain the answer\n",
    "\n",
    "Some of the above portions of the pipeline are implemented in an abstract fashion by some of the functions that we will see below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef1152",
   "metadata": {},
   "source": [
    "Let us load the documents using TextLoader from Langchain\n",
    "Let us also define a variable to store the documents (ideally a list because we can load up many docs and store them in a list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0efcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2394db65",
   "metadata": {},
   "source": [
    "loader.load() returns a list with one element in this case.\n",
    "\n",
    "Since we already have a defined list, we can append the output of loader.load() into our list variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934c1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(r'C:\\Users\\JkReddy\\Desktop\\Weill Cornell Medicine\\Subjects\\Capstone\\LangChain.txt')\n",
    "data.append(loader.load()[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a75a4d",
   "metadata": {},
   "source": [
    "Importing Vector DB - Chroma along with TextSplitter and QA related packages from Langchain. Also, import package for Embeddings from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80b16134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49875736",
   "metadata": {},
   "source": [
    "We split the lengthy document into smaller chunks. This is done because LLMs have a limit to the number of words they can take in as input. Also, they can retain more information in their \"memory\", recall and give accurate respones if they have lesser number of words to work with. \n",
    "\n",
    "Experiment with document chunk size parameter value below. Chunk overlap parameter dictates how much overlap should exist between the chunks. Having more of an overlap ensures that important information is not lost during the splitting process.\n",
    "\n",
    "split documents function takes in a list as an input. Each list element must contain a document loaded in by Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958c4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841be55d",
   "metadata": {},
   "source": [
    "Now that we have loaded up the text, we can now go ahead with creating a database, getting the embedding functions ready for embedding the docs and storing them in the vector db.\n",
    "\n",
    "In the below cell, we mention a directory in which we want our vector db to reside. This is then followed by the creation of an embeddings object from the OpenAIEmbeddings class from the Langchain package. This can used for creating the doc embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7524b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'myvectordb'\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c8a69",
   "metadata": {},
   "source": [
    "By default, the model used for embeddings is the text-ada-embeddings-002\n",
    "This model costs about $0.004 per 1000 tokens so be mindful\n",
    "\n",
    "The vector db we are using is Chroma DB which is integrated into Langchain\n",
    "\n",
    "**The Chroma.from_documents function takes in these parameters:**\n",
    "1. The split up texts\n",
    "2. embedding instance from Langchain\n",
    "3. directory in which we want the persistence of our db to be asserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d29edb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: myvectordb\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(texts, embeddings, persist_directory = persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d8424",
   "metadata": {},
   "source": [
    "Writing Embeddings to a disk using db.persist() and wiping it clean. Reload again to test if it has been stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d95aa771",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e031e2",
   "metadata": {},
   "source": [
    "Reload VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eba8fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: myvectordb\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function = embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54bfe48",
   "metadata": {},
   "source": [
    "Creating QA object with LLM model being passed as a parameter along with temperature parameter for controlling the nature of the LLM output. OpenAI class also takes in model_name as a parameter. Here we are going with davinci-003 so this is a bit pricey. For a cheaper alternative, go with ada or curie models of OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c5a5356",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature = 0.1, model_name = \"text-davinci-003\"), \n",
    "                                 chain_type = \"stuff\", \n",
    "                                 retriever = vectordb.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be98bf7",
   "metadata": {},
   "source": [
    "Our newly created qa object has the function query using which we can run a query over the db. Once the right doc chunk has been retrieved, it is passed to the llm along with the query. This helps us retrieve the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efbb1c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' You can eat collard greens, broccoli, yogurt, cheese, chia seeds, almonds, and sardines and other high-quality seafood to boost your calcium intake during pregnancy.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What can I eat?\"\n",
    "gpt_qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb408780",
   "metadata": {},
   "source": [
    "****\n",
    "Open Source Solution for the same task using llms available on hugging face.\n",
    "We use sentence-transformers embeddings for embedding the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6030738e",
   "metadata": {},
   "source": [
    "Creating an instance of an Open Source Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b4ee3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43b6beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4081130b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 510, which is longer than the specified 500\n",
      "Created a chunk of size 979, which is longer than the specified 500\n",
      "Created a chunk of size 518, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12f930b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: myvectordb_opensource\n"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "persist_directory = 'myvectordb_opensource'\n",
    "vectordb = Chroma.from_documents(texts, embeddings, persist_directory = persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f0683d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4606c",
   "metadata": {},
   "source": [
    "Currently, Langchain HF pipeline only supports models in the hub which function as text2text gen or text gen models. So be mindful when picking the model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ead75fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c258bf63b74596985cd5124e55165d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device has 1 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(model_id=\"declare-lab/flan-gpt4all-xl\", \n",
    "                                        task=\"text2text-generation\", \n",
    "                                        model_kwargs={\"temperature\":0, \"max_length\":50, \"min_length\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f42b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm, \n",
    "                                 chain_type = \"refine\", \n",
    "                                 retriever = vectordb.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0118c0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python38\\lib\\site-packages\\transformers\\generation\\utils.py:1190: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, you can have fruits. Chia seeds are a good source of calcium, fiber, and omega-3 fatty acids. There are many ways to use chia seeds, including chia pudding, added to energy bites'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Can I have fruits?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db10932",
   "metadata": {},
   "source": [
    "Truth be told, GPT by Open AI still seems to be better than most models at being chatty and accurate. However, if you do have the LLama Weights, I implore you to try the Vicuna model as I have heard that it delivers promisng results in this particular use case.\n",
    "****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
